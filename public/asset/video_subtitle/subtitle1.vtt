WEBVTT

1
00:00:06.335 --> 00:00:08.504
In the coming years,
artificial intelligence

2
00:00:08.504 --> 00:00:12.508
is probably going to change your life,
and likely the entire world.

3
00:00:12.592 --> 00:00:15.720
But people have a hard time
agreeing on exactly how.

4
00:00:15.720 --> 00:00:18.931
The following are excerpts
from a World Economic Forum interview

5
00:00:18.931 --> 00:00:22.727
where renowned computer science professor
and AI expert Stuart Russell

6
00:00:22.727 --> 00:00:25.313
helps separate the sense
from the nonsense.

7
00:00:25.313 --> 00:00:29.067
There’s a big difference between asking
a human to do something

8
00:00:29.067 --> 00:00:32.528
and giving that as the objective
to an AI system.

9
00:00:32.528 --> 00:00:35.156
When you ask a human to get
you a cup of coffee,

10
00:00:35.156 --> 00:00:37.742
you don’t mean this should be
their life’s mission,

11
00:00:37.742 --> 00:00:39.702
and nothing else in the universe matters.

12
00:00:39.702 --> 00:00:42.288
Even if they have to kill everybody else
in Starbucks

13
00:00:42.288 --> 00:00:45.124
to get you the coffee before it closes—
they should do that.

14
00:00:45.124 --> 00:00:46.751
No, that’s not what you mean.

15
00:00:46.751 --> 00:00:49.045
All the other things that
we mutually care about,

16
00:00:49.045 --> 00:00:51.214
they should factor
into your behavior as well.

17
00:00:51.214 --> 00:00:54.383
And the problem with the way
we build AI systems now

18
00:00:54.383 --> 00:00:56.010
is we give them a fixed objective.

19
00:00:56.010 --> 00:00:59.555
The algorithms require us
to specify everything in the objective.

20
00:00:59.555 --> 00:01:02.975
And if you say, can we fix the
acidification of the oceans?

21
00:01:02.975 --> 00:01:07.021
Yeah, you could have a catalytic reaction
that does that extremely efficiently,

22
00:01:07.021 --> 00:01:10.274
but it consumes a quarter
of the oxygen in the atmosphere,

23
00:01:10.274 --> 00:01:13.986
which would apparently cause us to die
fairly slowly and unpleasantly

24
00:01:13.986 --> 00:01:15.738
over the course of several hours.

25
00:01:15.780 --> 00:01:18.991
So, how do we avoid this problem?

26
00:01:18.991 --> 00:01:23.079
You might say, okay, well, just be more
careful about specifying the objective—

27
00:01:23.079 --> 00:01:25.623
don’t forget the atmospheric oxygen.

28
00:01:25.873 --> 00:01:29.418
And then, of course, some side effect
of the reaction in the ocean

29
00:01:29.418 --> 00:01:30.795
poisons all the fish.

30
00:01:30.795 --> 00:01:33.464
Okay, well I meant don’t kill
the fish either.

31
00:01:33.464 --> 00:01:35.383
And then, well, what about
the seaweed?

32
00:01:35.383 --> 00:01:38.344
Don’t do anything that’s going
to cause all the seaweed to die.

33
00:01:38.344 --> 00:01:39.554
And on and on and on.

34
00:01:39.679 --> 00:01:43.599
And the reason that we don’t have to do
that with humans is that

35
00:01:43.599 --> 00:01:48.104
humans often know that they don’t know
all the things that we care about.

36
00:01:48.354 --> 00:01:51.315
If you ask a human to get you
a cup of coffee,

37
00:01:51.315 --> 00:01:54.193
and you happen to be
in the Hotel George Sand in Paris,

38
00:01:54.193 --> 00:01:56.821
where the coffee is 13 euros a cup,

39
00:01:56.821 --> 00:02:00.992
it’s entirely reasonable to come
back and say, well, it’s 13 euros,

40
00:02:00.992 --> 00:02:03.953
are you sure you want it,
or I could go next door and get one?

41
00:02:03.953 --> 00:02:06.831
And it’s a perfectly normal thing
for a person to do.

42
00:02:07.039 --> 00:02:10.042
To ask, I’m going to repaint your house—

43
00:02:10.042 --> 00:02:13.379
is it okay if I take off the drainpipes
and then put them back?

44
00:02:13.504 --> 00:02:16.632
We don't think of this as a terribly
sophisticated capability,

45
00:02:16.632 --> 00:02:19.719
but AI systems don’t have it
because the way we build them now,

46
00:02:19.719 --> 00:02:21.512
they have to know the full objective.

47
00:02:21.721 --> 00:02:25.474
If we build systems that know that
they don’t know what the objective is,

48
00:02:25.474 --> 00:02:28.060
then they start to exhibit
these behaviors,

49
00:02:28.060 --> 00:02:32.106
like asking permission before getting rid
of all the oxygen in the atmosphere.

50
00:02:32.565 --> 00:02:35.943
In all these senses,
control over the AI system

51
00:02:35.943 --> 00:02:40.406
comes from the machine’s uncertainty
about what the true objective is.

52
00:02:41.282 --> 00:02:44.368
And it’s when you build machines that
believe with certainty

53
00:02:44.368 --> 00:02:45.786
that they have the objective,

54
00:02:45.786 --> 00:02:48.539
that’s when you get this
sort of psychopathic behavior.

55
00:02:48.539 --> 00:02:50.666
And I think we see
the same thing in humans.

56
00:02:50.750 --> 00:02:55.004
What happens when general purpose AI
hits the real economy?

57
00:02:55.379 --> 00:02:58.966
How do things change? Can we adapt?

58
00:02:59.175 --> 00:03:01.010
This is a very old point.

59
00:03:01.010 --> 00:03:04.597
Amazingly, Aristotle actually has
a passage where he says,

60
00:03:04.597 --> 00:03:07.642
look, if we had fully automated
weaving machines

61
00:03:07.642 --> 00:03:11.479
and plectrums that could pluck the lyre
and produce music without any humans,

62
00:03:11.604 --> 00:03:13.606
then we wouldn’t need any workers.

63
00:03:13.814 --> 00:03:16.692
That idea, which I think it was Keynes

64
00:03:16.692 --> 00:03:19.528
who called it technological unemployment
in 1930,

65
00:03:19.528 --> 00:03:21.447
is very obvious to people.

66
00:03:21.447 --> 00:03:24.533
They think, yeah, of course,
if the machine does the work,

67
00:03:24.533 --> 00:03:26.202
then I'm going to be unemployed.

68
00:03:26.369 --> 00:03:29.872
You can think about the warehouses
that companies are currently operating

69
00:03:29.872 --> 00:03:32.583
for e-commerce,
they are half automated.

70
00:03:32.583 --> 00:03:36.629
The way it works is that an old warehouse—
where you’ve got tons of stuff piled up

71
00:03:36.629 --> 00:03:39.090
all over the place
and humans go and rummage around

72
00:03:39.090 --> 00:03:40.967
and then bring it back and send it off—

73
00:03:40.967 --> 00:03:44.553
there’s a robot who goes
and gets the shelving unit

74
00:03:44.553 --> 00:03:46.472
that contains the thing that you need,

75
00:03:46.472 --> 00:03:50.101
but the human has to pick the object
out of the bin or off the shelf,

76
00:03:50.101 --> 00:03:51.978
because that’s still too difficult.

77
00:03:52.019 --> 00:03:54.021
But, at the same time,

78
00:03:54.021 --> 00:03:57.942
would you make a robot that is accurate
enough to be able to pick

79
00:03:57.942 --> 00:04:02.280
pretty much any object within a very wide
variety of objects that you can buy?

80
00:04:02.280 --> 00:04:06.284
That would, at a stroke,
eliminate 3 or 4 million jobs?

81
00:04:06.409 --> 00:04:09.745
There's an interesting story
that E.M. Forster wrote,

82
00:04:09.745 --> 00:04:13.249
where everyone is entirely
machine dependent.

83
00:04:13.499 --> 00:04:17.253
The story is really about the
fact that if you hand over

84
00:04:17.253 --> 00:04:20.214
the management of your civilization
to machines,

85
00:04:20.214 --> 00:04:23.718
you then lose the incentive to understand
it yourself

86
00:04:23.718 --> 00:04:26.262
or to teach the next generation
how to understand it.

87
00:04:26.262 --> 00:04:29.265
You can see “WALL-E”
actually as a modern version,

88
00:04:29.265 --> 00:04:32.893
where everyone is enfeebled
and infantilized by the machine,

89
00:04:32.893 --> 00:04:34.854
and that hasn’t been possible
up to now.

90
00:04:34.854 --> 00:04:37.273
We put a lot of our civilization
into books,

91
00:04:37.273 --> 00:04:38.899
but the books can’t run it for us.

92
00:04:38.899 --> 00:04:41.694
And so we always have to teach
the next generation.

93
00:04:41.736 --> 00:04:45.948
If you work it out, it’s about a trillion
person years of teaching and learning

94
00:04:45.948 --> 00:04:49.910
and an unbroken chain that goes back
tens of thousands of generations.

95
00:04:50.119 --> 00:04:52.038
What happens if that chain breaks?

96
00:04:52.038 --> 00:04:55.499
I think that’s something we have
to understand as AI moves forward.

97
00:04:55.624 --> 00:04:59.211
The actual date of arrival
of general purpose AI—

98
00:04:59.211 --> 00:05:02.298
you’re not going to be able to pinpoint,
it isn’t a single day.

99
00:05:02.298 --> 00:05:04.592
It’s also not the case
that it’s all or nothing.

100
00:05:04.592 --> 00:05:07.053
The impact is going to be increasing.

101
00:05:07.053 --> 00:05:09.096
So with every advance in AI,

102
00:05:09.096 --> 00:05:12.058
it significantly expands
the range of tasks.

103
00:05:12.058 --> 00:05:17.396
So in that sense, I think most experts say
by the end of the century,

104
00:05:17.396 --> 00:05:20.733
we’re very, very likely to have
general purpose AI.

105
00:05:20.733 --> 00:05:24.487
The median is something around 2045.

106
00:05:24.487 --> 00:05:26.489
I'm a little more on the
conservative side.

107
00:05:26.489 --> 00:05:28.574
I think the problem is
harder than we think.

108
00:05:28.574 --> 00:05:31.827
I like what John McAfee,
he was one of the founders of AI,

109
00:05:31.911 --> 00:05:35.748
when he was asked this question, he said,
somewhere between five and 500 years.

110
00:05:35.748 --> 00:05:39.085
And we're going to need, I think, several
Einsteins to make it happen.

